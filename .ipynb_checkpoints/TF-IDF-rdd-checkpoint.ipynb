{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "import sys,re\n",
    "import math\n",
    "import string\n",
    "try:\n",
    "    from sets import Set\n",
    "except ImportError:\n",
    "    Set = set\n",
    "import numpy # pip install numpy\n",
    "\n",
    "\n",
    "from pyspark import SparkContext # spark 1.x\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib import *\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "Change the following setting according to your machine environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_nn = \"127.0.0.1\" # TODO: change it if it differs from your machine environment\n",
    "hdfs_nn = \"local\" # Remove this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Improving SVM classifier with stopwords removal and N-Gram \n",
    "\n",
    "\n",
    "Checking through the data, we find that all the data are already in lower case. However there are still some punctuations. Let’s remove them. You may refer to the following code snippets to help you remove punctuation when building your Spark model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(tweet):\n",
    "    return re.sub('[\\\"\\'.,!#@%\\[\\]{}*^]','',tweet) # OLD ONE WAS GIVING POOR RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are a lot of stop words such as \"I\", \"me\", \"to\", \"in\". Let’s remove them. You may refer to the following code snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = Set(['and', 'is', 'it', 'are', 'in', 'rt', 'what',\\\n",
    "   'from', 'her', 'to', 'their', 'you', 'me', 'his', 'http', 'that',\\\n",
    "   'they', 'by', 'he', 'a', 'on', 'for', 'i', 'of', 'this', 'she', 'the', 'my', 'at'])\n",
    "\n",
    "\n",
    "def remove_stop_words(words):\n",
    "    return [w for w in words if (w not in stop_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The following are some other helper functions that we defined in practical 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_fixed_size = 30 # fixed the size of each vector.\n",
    "# if vectors have different sizes, the gradient descent algorithm will fail\n",
    "# cut off if it exceeds, pad zeros if it has less than 30 elements\n",
    "\n",
    "\n",
    "def hash(str):\n",
    "    return reduce(lambda h,c:numpy.int32(31*h+ord(c)), str, 2147483647)\n",
    "\n",
    "def to_words(tweet):\n",
    "    return remove_stop_words(filter(None, remove_punct(tweet).split(\" \"))) # Added a filter here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NGrams \n",
    "\n",
    "One observation is that some special korean idol names such as \"ahn jae hyun\" and \"yoo in na\", it is hard to use them individually. We may want to consider 2-ngrams or 3-grams as the terms of the language model. \n",
    "\n",
    "Define a `two_grams` function and a `three_grams` function, which later can be used to replace the `to_words` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "            SparkSession.builder.master(hdfs_nn)\n",
    "            .config(\"spark.io.compression.codec\", \"snappy\")\n",
    "            .config(\"spark.ui.enabled\", \"false\")\n",
    "            .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "s = \"The virus that causes COVID-19 is mainly  transmitted through droplets\"\n",
    "\n",
    "def two_grams(str):\n",
    "    words = to_words(str)\n",
    "    return [\" \".join([words[i], words[i+1]]) for i in range(len(words)-1)] # TODO: fixme \n",
    "\n",
    "\n",
    "def three_grams(str):\n",
    "    words = to_words(str)\n",
    "    return [\" \".join([words[i], words[i+1], words[i+2]]) for i in range(len(words)-2)] # TODO: fixme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test cases for `two_grams` and `three_grams`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The virus', 'virus causes', 'causes COVID-19', 'COVID-19 mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n",
      "['The virus causes', 'virus causes COVID-19', 'causes COVID-19 mainly', 'COVID-19 mainly transmitted', 'mainly transmitted through', 'transmitted through droplets']\n"
     ]
    }
   ],
   "source": [
    "s = \"The virus that causes COVID-19 is mainly  transmitted through droplets\"\n",
    "\n",
    "print(two_grams(s))\n",
    "print(three_grams(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Expected output\n",
    "\n",
    "```text\n",
    "['The virus', 'virus causes', 'causes COVID-19', 'COVID-19 mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n",
    "['The virus causes', 'virus causes COVID-19', 'causes COVID-19 mainly', 'COVID-19 mainly transmitted', 'mainly transmitted through', 'transmitted through droplets']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training the model\n",
    "\n",
    "Let's incorporate the `two_grams` and `three_grams` functions into our SVM model. First we need to load the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "posTXT = sc.textFile(\"Tweet_data/Kpop/*.txt\")\n",
    "negTXT = sc.textFile(\"Tweet_data/othertweet/*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply two_grams to construct the positive and negative labelled points and build a SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def pad_cap(xs,size):\n",
    "    return xs[0:size] + [ 0.0 for x in range(0, size-len(xs))]\n",
    "\n",
    "    \n",
    "posTerms = posTXT.map(lambda line: two_grams(line))\n",
    "negTerms = negTXT.map(lambda line: two_grams(line))\n",
    "terms = posTerms + negTerms\n",
    "\n",
    "\n",
    "posLP = posTerms.map(lambda terms:  LabeledPoint(1.0, Vectors.dense(pad_cap(list(map(lambda w:hash(w), terms)),vector_fixed_size))))\n",
    "negLP = negTerms.map(lambda terms:  LabeledPoint(0.0, Vectors.dense(pad_cap(list(map(lambda w:hash(w), terms)),vector_fixed_size))))\n",
    "data = negLP + posLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We split the data into training and testing, feed the training into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = data.randomSplit([0.6,0.4],seed = 11)\n",
    "training = splits[0].cache()\n",
    "test = splits[1]\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "num_iteration = 20\n",
    "num_iteration = 10 # REMOVE THIS\n",
    "model = SVMWithSGD.train(training,num_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "We apply the model to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the default threshold\n",
    "model.clearThreshold()\n",
    "# Compute raw scores on the test set\n",
    "score_and_labels = test.map( lambda point: (float(model.predict(point.features)), point.label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We evaluate the performance of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.49537914375700626\n"
     ]
    }
   ],
   "source": [
    "# Get the evaluation metrics\n",
    "metrics = BinaryClassificationMetrics(score_and_labels)\n",
    "au_roc = metrics.areaUnderROC\n",
    "\n",
    "print(\"Area under ROC = %s\" % str(au_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have auROC is around 0.49 to 0.62.  You may try to use `three_grams()` or improve the stop word set and the punctuation removal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fixing the performance bugs\n",
    "\n",
    "Something is still not quite right. \n",
    "\n",
    "We've tried a few attempts but the improvement of our SVM model is puny. We gain less than 0.1 in terms of area under ROC. To understand why it is the case, we have to think about how SVM works. SVM expects each training sample is a vector. It plots all the positive and negative training samples in the vector space and try to derive a polynomal border to segment the positive and the negatives. However the way we built the vectors from the tweets in the earlier section is volatile under the orders of words and easily affected by noise. For instance, \"I love yoo jae suk\" and \"yoo jae suk is in Singapore\" are two possible KPOP related tweets. However, they result in two different vectors using the `hash` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love', 'love yoo', 'yoo jae', 'jae suk']\n",
      "[-912178776, 2106175338, -595126104, -1432813480]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(two_grams(\"I love yoo jae suk\"))\n",
    "print(list(map(hash, two_grams(\"I love yoo jae suk\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yoo jae', 'jae suk', 'suk Singapore']\n",
      "[-595126104, -1432813480, 700126286]\n"
     ]
    }
   ],
   "source": [
    "print(two_grams(\"yoo jae suk is in Singapore\"))\n",
    "print(list(map(hash, two_grams(\"yoo jae suk is in Singapore\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The common values `-595126104`, `-1432813480`, (corespondent to \"yoo jae\" and \"jae suk\") are found in different positions within the two vectors. As a result they are in different dimension of the vector space. It is harder to derive a good polynomial border that seperate these two vectors with the non-Kpop training sample. It would be a lot easier if these two vectors are \"nearer\" to each other. It seems to be making sense to \"align\" the vector dimensions based on all the words or grams. One possibility is to sort all the words arising in the training data. However this approach is not practical because that would cause an explosion in size in terms of the vector space.\n",
    "\n",
    "Fortunately, there is a well known technique called *TF-IDF* to find out the \"important\" terms appearing on the corpus. \n",
    "\n",
    "### TF-IDF saves the day\n",
    "\n",
    "TF-IDF stands for Term frequency - Inverse document frequency. It is a classic and powerful technique to search for interesting terms from a set\n",
    "of documents.\n",
    "\n",
    "*  TF is actually the word count. For instance, consider the following text data.\n",
    "```text\n",
    "apple smart phone\n",
    "android smart phone\n",
    "```\n",
    "We assume that each line is a document, hence there are two documents here.\n",
    "\n",
    "* The term frequency is\n",
    "```text\n",
    "apple, 1\n",
    "android, 1\n",
    "phone, 2\n",
    "smart, 2\n",
    "```\n",
    "\n",
    "* IDF is is the total number of documents/records divided by the total number of the documents/records containing the words. We apply logarithmic to the quotient. The IDF for the above example is\n",
    "```text\n",
    "apple, log(2/1)\n",
    "android, log(2/1)\n",
    "phone, log(2/2)\n",
    "smart, log(2/2)\n",
    "```\n",
    "that is\n",
    "```text\n",
    "apple, 0.693\n",
    "android, 0.693\n",
    "phone, 0\n",
    "smart, 0\n",
    "```\n",
    "\n",
    "* TF-IDF is obtained by multiplying the TF with the IDF.\n",
    "```text\n",
    "apple, 0.693\n",
    "android, 0.693\n",
    "phone, 0\n",
    "smart, 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define the term frequency function `tf` \n",
    "\n",
    "Hint: `tf` is actually the word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tf(terms):\n",
    "    word_count = terms.flatMap(list).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    return word_count # TODO: fixme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('apple', 1), ('smart', 2), ('phone', 2), ('android', 1)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def one_grams(s):\n",
    "    return s.split(\" \")\n",
    "\n",
    "test_terms = [one_grams(\"apple smart phone\"), one_grams(\"android smart phone\")]\n",
    "# print(test_terms)\n",
    "test_tf = tf(sc.parallelize(test_terms))\n",
    "test_tf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Expected output\n",
    "\n",
    "```text\n",
    "[('phone', 2), ('android', 1), ('apple', 1), ('smart', 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Define the document frequency function `df`\n",
    "\n",
    "Hint: `df` is almost the same as `tf` except that duplications within a record (a document) are disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df(terms):\n",
    "    word_count = terms.flatMap(lambda x: list(set(x))).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y)\n",
    "    return word_count # TODO: fixme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smart', 2), ('phone', 2), ('apple', 1), ('android', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_df = df(sc.parallelize(test_terms))\n",
    "test_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining `tf` and `df`\n",
    "\n",
    "We now can define `tfidf` in terms of `tf` and `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tfidf(terms): \n",
    "    dCount = terms.count()\n",
    "    tfreq = tf(terms)\n",
    "    dfreq = df(terms)\n",
    "    return tfreq.join(dfreq).map(lambda p :(p[0], p[1][0] * math.log(dCount/p[1][1]))).sortBy( lambda p : - p[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's incorporate that into our Spark machine learning model. For instance, we can run tf-idf, sort the terms according to tf-idf score in descending order, and collect the top 150\n",
    "terms, and use them as the vector dimensions. (This implies that we will have a 150-dimension space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jae suk', 'yoo jae', 'jae lee', 'kim jae', 'jae millz', 'ahn jae', 'jae hyun', 'jae joong', 'running man', 'yoon jae', 'with jae', 'song jae', 'jeremiah jae', 'jae crowder', 'young jae', 'jae wook', 'liked video', 'jae kyung', 'lee jae', 'jae hee', 'jae rim', 'jae lees', 'jae hyeon', 'hyuk jae', 'cover jae', 'jae kim', 'check out', 'jae jung', 'jung jae', 'happy bihday', 'jae hoon', 'jae jin', 'park jae', 'will be', 'like jae', 'jae park', 'lee jong', 'jae suks', 'jae joon', 'thanks jae', 'jae min', 'lee hyuk', 'south korea', 'jae shin', 'lee hwi', 'looks like', 'jaehoon ha', 'joong 1st', 'jae yoo', 'lee jungjae', 'jae jae', 'has been', 'yoo jaesuk', 'jae bum', 'jung joon', 'ho jae', 'millz troy', 'jong kook', 'photo jae', 'right now', 'joon young', 'jae ari', 'jae hyung', 'lee young', 'asia tour', 'new favorite', 'jae rhim', 'jae won', 'jae yeong', 'new music', 'so eun', 'c hong', 'yong jae', 'tour conce', 'jae yong', 'jae wan', 'sketch cover', 'video playlist', 'greg pak', 'added video', 'yeong hwang', 'sung jae', '1st album', 'kim so', 'parc jae', '(parc jae', 'hun lee', 'app store', 'kwang soo', 'si jae', 'love jae', 'ji hyo', 'ahn jung', 'jae woong', 'hwi jae', 'jong suk', 'jae hun', 'suk jin', 'ice baby', 'new mcs', 'album asia', 'millz og', 'free app', 'min joon', 'encryptor jae', 'text encryptor', 'download via', 'jae jung)', 'store download', 'jae ga', 'de la', 'out jae', 'now free', 'drawn sketch', 'jae seo', 'dont know', 'young ahn', 'sungha jung', 'wonder woman', 'millzy johnson', 'soo hyun', 'og millzy', 'kim jong', 'photo/jae c', 'would be', 'lee now', 'jae synth', 'han jae', 'suk ji', 'hyun jae', 'suk yoo', 'jong hyun', 'kwon jae', 'kim jaejoong', 'mushroom burial', 'hand drawn', 'do min', 'im jae', 'video jae', 'black white', 'so much', 'jae lol', 'cha jae', 'rhim lee', 'burial suit', 'follow enter', 'jyjs kim', 'music jae', '(ap/jae c', 'ahn jaehyun']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topTerms = list(map(lambda p:p[0], tfidf(terms).collect()[:150]))\n",
    "print(topTerms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expected output\n",
    "\n",
    "We should see something similar to the following \n",
    "\n",
    "```text\n",
    "[u'jae millz', u'jae suk', u'jae lee', u'yoo jae', u'jae crowder', u'jeremiah jae', u'kim jae', u'ahn jae', u'jae hyun', u'with jae', u'jae joong', u'liked video', u'check out', u'happy bihday', u'thanks jae', u'running man', u'jae jae', u'yoon jae', u'jae synth', u'new music', u'ho jae', u'directed jae', u'song jae', u'jae tips', u'will be', u'young jae', u'jae hood', u'jae hee', u'jae wook', u'lee jae', u'millz og', u'millzy johnson', u'og millzy', u'new favorite', u'like jae', u'name jae', u'jae kyung', u'jae ga', u'hi jae', u'cover jae', u'love jae', u'thank jae', u'young money', u'jae jin', u'jae rim', u'murda mook', u'jae kim', u'aesop rock', u'jae lees', u'prod jeremiah', u'music jae', u'danny brown', u'ego death', u'jasmine jae', u'rock danny', u'ft jae', u'out jae', u'jaesean tate', u'video jae', u'bihday jae', u'brown prod', u'busdriver ego', u'jung jae', u'hyuk jae', u'millz troy', u'jae mills', u'im jae', u'gudda gudda', u'right now', u'park jae', u'jae hoon', u'jae im', u'has been', u'looks like', u'jae jung', u'travoltified name', u'jae so', u'jae lol', u'jae ari', u'jae hyeon', u'jae has', u'new video', u'when jae', u'feat jae', u'photo jae', u'best friend', u'\\xc3\\xbf \\xc3\\xbf', u'vs jae', u'would be', u'so much', u'nigga jae', u'see jae', u'(official music', u'jae won', u'jae )', u'music video)', u'jae said', u'feat aesop', u'think jae', u'jae park', u'millz vs', u'or jae', u'jae joon', u'miss jae', u'death feat', u'millz 1990', u'jae wan', u'sarah jae', u'added video', u'whats yours', u'si jae', u'video playlist', u'jaehoon ha', u'baby jae', u'about jae', u'jae bae', u'jae min', u'lil wayne', u'better than', u'jae foster', u'music video', u'jae just', u'im not', u'no one', u'mook vs', u'jae suks', u'(official video)', u'top 3', u'interview with', u'south korea', u'lee jong', u'cha jae', u'jae yoo', u'do jae', u'lol jae', u'jae (', u'good times', u'lee jungjae', u'suk jin', u'dont know', u'than jae', u'yoo jaesuk', u'jae bum', u'jae good', u'3 aists', u'jae laffer', u'lee hwi', u'jae shin', u'gudda jae', u'jae busdriverse']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recalling our counter example, \n",
    "\n",
    "```text\n",
    "I love yoo jae suk\n",
    "```\n",
    "\n",
    "If we use the above `topTerms` as a reference, we build a 150-element vector from the two gram of \"I love yoo jae suk\", it should yield\n",
    "\n",
    "```text\n",
    "Vector(0.0, 0.0, 1.0, 0.0, ... )\n",
    "```\n",
    "\n",
    "Because the 3rd term \"jae suk\" from the top 150 tfidf is present in the tweet, the 3rd number in the vector is 1.0 while the rest are zeros.\n",
    "\n",
    "Similarly, the vector for \"yoo jae suk is in Singapore\" will have the vector\n",
    "\n",
    "```text\n",
    "Vector(0.0, 0.0, 1.0, 0.0, ... )\n",
    "```\n",
    "\n",
    "\n",
    "### Defining the function `ComputeLP` to create labeled points (3 Marks)\n",
    "\n",
    "Let's define a function to convert a tweet into a labeled point, recalling that labeled point is the numeric label (0 or 1) and the vector representation of our data.\n",
    "\n",
    "Concretely speaking, the `computeLP` function takes a label `1.0` or `0.0`, a sequence of string i.e. the 2-grams or 3-grams, and a array of top-N TF-IDF.\n",
    "\n",
    "For each tf-idf term, let's say `t` is the i-th top-N TF-IDF term, if `t` is in the sequence of strings, we should put a `1.0` at the i-th position of the output vector, otherwise it should be `0.0`.\n",
    "\n",
    "Complete the following function according to the example given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLP(label, n_grams_text, tf_idf_text):\n",
    "    return LabeledPoint(label, Vectors.dense([1.0 if r in n_grams_text else 0.0 for r in tf_idf_text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeLP(1.0, two_grams(\"I love yoo jae suk\"), topTerms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Expected output\n",
    "\n",
    "```text\n",
    "LabeledPoint(1.0, [0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We incorporate the above `computeLP()` into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posLP = posTerms.map( lambda seq: computeLP(1.0, seq, topTerms) )\n",
    "negLP = negTerms.map( lambda seq: computeLP(0.0, seq, topTerms) )\n",
    "\n",
    "data = negLP + posLP\n",
    "\n",
    "\n",
    "# Split data into training (60%) and test (40%).\n",
    "\n",
    "splits = data.randomSplit([0.6,0.4],seed = 11)\n",
    "training = splits[0].cache()\n",
    "test = splits[1]\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "num_iteration = 100\n",
    "num_iteration = 10 # REMOVE THIS\n",
    "\n",
    "model = SVMWithSGD.train(training,num_iteration)\n",
    "\n",
    "# This will takes about 20 mins on a 4-core intel i7 processor 3.8GHZ with hyperthreading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We apply the updated model to our testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clearThreshold()\n",
    "# Compute raw scores on the test set\n",
    "score_and_labels = test.map( lambda point: (float(model.predict(point.features)), point.label) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We would like to re-evaluate the performance of the new model incorporated with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.8901226000462642\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the evaluation metrics\n",
    "metrics = BinaryClassificationMetrics(score_and_labels)\n",
    "au_roc = metrics.areaUnderROC\n",
    "\n",
    "print(\"Area under ROC = %s\" % str(au_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see a significant increase in auROC, which is about 0.8 to 0.88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
